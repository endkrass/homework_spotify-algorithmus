{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Usage & Final Model Save\n",
        "\n",
        "This notebook covers:\n",
        "- Phase 10: Final Model Save and Usage Examples\n",
        "\n",
        "**Note:** Run `pipeline_gridsearch.ipynb` first to get the optimized model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_STATE = 777\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FINAL MODEL - TRAINING ON ALL DATA\n",
            "============================================================\n",
            "Training on all 21585 samples\n"
          ]
        }
      ],
      "source": [
        "# Load preprocessed data and optimized model\n",
        "with open('models/preprocessor.pkl', 'rb') as f:\n",
        "    preprocessor = pickle.load(f)\n",
        "\n",
        "# Load optimized model from pipeline_gridsearch.ipynb\n",
        "# In practice, you'd load the grid_search.best_estimator_ from that notebook\n",
        "# For demonstration, we'll create a simple example\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL MODEL - TRAINING ON ALL DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Reload and prepare all data\n",
        "df = pd.read_csv('data/spotify-tracks.csv')\n",
        "columns_to_drop = ['spotify_id', 'name', 'artists', 'album_name', 'album_release_date',\n",
        "                   'popular_in_country', 'mode', 'is_explicit', 'release_year', \n",
        "                   'key', 'time_signature']\n",
        "df_clean = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "target = 'energy'\n",
        "y = df_clean[target].copy()\n",
        "X = df_clean.drop(columns=[target]).copy()\n",
        "\n",
        "# Feature engineering\n",
        "# Only valid interaction features (no target leakage, no data leakage)\n",
        "X_engineered = X.copy()\n",
        "X_engineered['loudness_tempo'] = X_engineered['loudness'] * X_engineered['tempo']\n",
        "X_engineered['danceability_valence'] = X_engineered['danceability'] * X_engineered['valence']\n",
        "X_engineered['loudness_danceability'] = X_engineered['loudness'] * X_engineered['danceability']\n",
        "X_engineered['tempo_valence'] = X_engineered['tempo'] * X_engineered['valence']\n",
        "X = X_engineered.copy()\n",
        "\n",
        "# Preprocess all data\n",
        "X_all_processed = preprocessor.fit_transform(X)\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "print(f\"Training on all {len(X_all_processed)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded top_10_indices: [3, 11, 4, 14, 15, 6, 9, 5, 0, 1]\n",
            "\n",
            "Final Model Performance (all data):\n",
            "  R²: 0.7793\n"
          ]
        }
      ],
      "source": [
        "# Train final model on all data\n",
        "# Note: In practice, load the optimized model from pipeline_gridsearch.ipynb\n",
        "# For demonstration, we'll use GradientBoostingRegressor with good defaults\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Load top 10 indices from modelling.ipynb\n",
        "try:\n",
        "    with open('models/top_10_indices.pkl', 'rb') as f:\n",
        "        top_10_indices = pickle.load(f)\n",
        "    print(f\"✅ Loaded top_10_indices: {top_10_indices}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"⚠️ top_10_indices.pkl not found. Run modelling.ipynb first!\")\n",
        "    print(\"   Using first 10 features as fallback...\")\n",
        "    top_10_indices = list(range(10))\n",
        "\n",
        "X_all_top10 = X_all_processed[:, top_10_indices]\n",
        "\n",
        "# Train final model\n",
        "final_model = GradientBoostingRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.9,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "final_model.fit(X_all_top10, y)\n",
        "\n",
        "# Verify it works\n",
        "y_pred_all = final_model.predict(X_all_top10)\n",
        "r2_all = r2_score(y, y_pred_all)\n",
        "\n",
        "print(f\"\\nFinal Model Performance (all data):\")\n",
        "print(f\"  R²: {r2_all:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "SAVING MODEL\n",
            "============================================================\n",
            "✅ Model saved to 'models/model.pkl'\n"
          ]
        }
      ],
      "source": [
        "# Save model package\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAVING MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "model_package = {\n",
        "    'model': final_model,\n",
        "    'top_10_indices': top_10_indices,\n",
        "    'feature_names': feature_names,\n",
        "    'preprocessor': preprocessor,\n",
        "    'model_type': type(final_model).__name__,\n",
        "    'r2_score': r2_all\n",
        "}\n",
        "\n",
        "import os\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "with open('models/model.pkl', 'wb') as f:\n",
        "    pickle.dump(model_package, f)\n",
        "\n",
        "print(\"✅ Model saved to 'models/model.pkl'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING MODEL LOADING\n",
            "============================================================\n",
            "✅ Model loaded successfully\n",
            "   Model type: GradientBoostingRegressor\n",
            "   R² score: 0.7793\n"
          ]
        }
      ],
      "source": [
        "# Test loading\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TESTING MODEL LOADING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with open('models/model.pkl', 'rb') as f:\n",
        "    loaded_package = pickle.load(f)\n",
        "\n",
        "loaded_model = loaded_package['model']\n",
        "loaded_indices = loaded_package['top_10_indices']\n",
        "loaded_preprocessor = loaded_package['preprocessor']\n",
        "\n",
        "print(\"✅ Model loaded successfully\")\n",
        "print(f\"   Model type: {loaded_package['model_type']}\")\n",
        "print(f\"   R² score: {loaded_package['r2_score']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING PREDICTIONS\n",
            "============================================================\n",
            "\n",
            "Sample predictions:\n",
            "Actual values:    [0.666 0.639 0.802 0.917 0.592]\n",
            "Predicted values: [0.6616102  0.65444796 0.81849333 0.86394187 0.54749   ]\n",
            "\n",
            "Range check:\n",
            "  Min prediction: 0.547\n",
            "  Max prediction: 0.864\n",
            "✅ All predictions in valid range [0, 1]\n"
          ]
        }
      ],
      "source": [
        "# Test prediction\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TESTING PREDICTIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Use a sample from the dataset\n",
        "test_sample = X.iloc[:5].copy()\n",
        "test_sample_processed = loaded_preprocessor.transform(test_sample)\n",
        "test_sample_top10 = test_sample_processed[:, loaded_indices]\n",
        "\n",
        "predictions = loaded_model.predict(test_sample_top10)\n",
        "\n",
        "print(\"\\nSample predictions:\")\n",
        "print(f\"Actual values:    {y.iloc[:5].values}\")\n",
        "print(f\"Predicted values: {predictions}\")\n",
        "print(f\"\\nRange check:\")\n",
        "print(f\"  Min prediction: {predictions.min():.3f}\")\n",
        "print(f\"  Max prediction: {predictions.max():.3f}\")\n",
        "\n",
        "if all(0 <= p <= 1 for p in predictions):\n",
        "    print(\"✅ All predictions in valid range [0, 1]\")\n",
        "else:\n",
        "    print(\"⚠️ Some predictions outside [0, 1] range\")\n",
        "    print(\"   Consider clipping predictions\")\n",
        "    \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
