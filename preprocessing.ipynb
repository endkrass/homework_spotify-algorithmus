{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing\n",
        "\n",
        "This notebook covers:\n",
        "- Phase 4: Preprocessing Pipeline\n",
        "\n",
        "**Note:** Run `exploration.ipynb` first to prepare the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_STATE = 777\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded: 21585 samples, 17 features\n"
          ]
        }
      ],
      "source": [
        "# Load and prepare data (same as exploration.ipynb)\n",
        "df = pd.read_csv('data/spotify-tracks.csv')\n",
        "\n",
        "# Remove non-predictive columns\n",
        "columns_to_drop = ['spotify_id', 'name', 'artists', 'album_name', 'album_release_date',\n",
        "                   'popular_in_country', 'mode', 'is_explicit', 'release_year', \n",
        "                   'key', 'time_signature']\n",
        "df_clean = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "# Define target\n",
        "target = 'energy'\n",
        "y = df_clean[target].copy()\n",
        "X = df_clean.drop(columns=[target]).copy()\n",
        "\n",
        "# Feature engineering (from exploration.ipynb)\n",
        "# Only valid interaction features (no target leakage, no data leakage)\n",
        "X_engineered = X.copy()\n",
        "\n",
        "# Interaction features\n",
        "X_engineered['loudness_tempo'] = X_engineered['loudness'] * X_engineered['tempo']\n",
        "X_engineered['danceability_valence'] = X_engineered['danceability'] * X_engineered['valence']\n",
        "X_engineered['loudness_danceability'] = X_engineered['loudness'] * X_engineered['danceability']\n",
        "X_engineered['tempo_valence'] = X_engineered['tempo'] * X_engineered['valence']\n",
        "\n",
        "X = X_engineered.copy()\n",
        "\n",
        "print(f\"Data loaded: {X.shape[0]} samples, {X.shape[1]} features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 4: Preprocessing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numeric columns: 16\n",
            "Categorical columns: 0\n"
          ]
        }
      ],
      "source": [
        "# Define feature types\n",
        "numeric_features = [\n",
        "    'danceability', 'liveness', 'tempo', 'loudness', 'speechiness',\n",
        "    'duration_ms', 'instrumentalness', 'popularity', 'dynamic_range',\n",
        "    'valence', 'rhythmic_complexity', 'acousticness',\n",
        "    'release_month'\n",
        "]\n",
        "\n",
        "categorical_features = [\n",
        "    # Note: Most categorical features were dropped\n",
        "]\n",
        "\n",
        "# Get all numeric columns (including engineered features)\n",
        "numeric_cols = [col for col in X.columns if X[col].dtype in ['int64', 'float64'] \n",
        "                and col not in categorical_features]\n",
        "\n",
        "categorical_cols = [col for col in categorical_features if col in X.columns]\n",
        "\n",
        "print(f\"Numeric columns: {len(numeric_cols)}\")\n",
        "print(f\"Categorical columns: {len(categorical_cols)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TRAIN/TEST SPLIT\n",
            "============================================================\n",
            "Training set: 17268 samples\n",
            "Test set: 4317 samples\n",
            "Features: 17\n"
          ]
        }
      ],
      "source": [
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=None\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAIN/TEST SPLIT\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Features: {X_train.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "PREPROCESSING PIPELINE\n",
            "============================================================\n",
            "✅ Preprocessing pipelines created\n"
          ]
        }
      ],
      "source": [
        "# Create preprocessing pipelines\n",
        "print(\"=\" * 60)\n",
        "print(\"PREPROCESSING PIPELINE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Numeric pipeline: Median imputation + StandardScaler\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical pipeline: Most frequent imputation + OneHotEncoder\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "# ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "print(\"✅ Preprocessing pipelines created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fitting preprocessor on training data...\n",
            "\n",
            "✅ Preprocessing complete\n",
            "Training shape: (17268, 16)\n",
            "Test shape: (4317, 16)\n",
            "Total features after preprocessing: 16\n",
            "\n",
            "First 10 feature names: ['num__danceability', 'num__liveness', 'num__tempo', 'num__loudness', 'num__speechiness', 'num__duration_ms', 'num__instrumentalness', 'num__popularity', 'num__dynamic_range', 'num__valence']\n"
          ]
        }
      ],
      "source": [
        "# Fit and transform\n",
        "print(\"\\nFitting preprocessor on training data...\")\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"\\n✅ Preprocessing complete\")\n",
        "print(f\"Training shape: {X_train_processed.shape}\")\n",
        "print(f\"Test shape: {X_test_processed.shape}\")\n",
        "print(f\"Total features after preprocessing: {X_train_processed.shape[1]}\")\n",
        "\n",
        "# Get feature names\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "print(f\"\\nFirst 10 feature names: {feature_names[:10].tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DATA LEAKAGE CHECK\n",
            "============================================================\n",
            "✅ Preprocessor fitted only on training data\n",
            "✅ Test set transformed separately\n",
            "✅ No target variable in preprocessing pipeline\n"
          ]
        }
      ],
      "source": [
        "# Verify no data leakage\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA LEAKAGE CHECK\")\n",
        "print(\"=\" * 60)\n",
        "print(\"✅ Preprocessor fitted only on training data\")\n",
        "print(\"✅ Test set transformed separately\")\n",
        "print(\"✅ No target variable in preprocessing pipeline\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Preprocessor saved to 'models/preprocessor.pkl'\n"
          ]
        }
      ],
      "source": [
        "# Save preprocessor\n",
        "import os\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "with open('models/preprocessor.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessor, f)\n",
        "\n",
        "print(\"✅ Preprocessor saved to 'models/preprocessor.pkl'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook we:\n",
        "1. ✅ Split data into train/test sets (80/20)\n",
        "2. ✅ Created preprocessing pipelines:\n",
        "   - Numeric: Median imputation + StandardScaler\n",
        "   - Categorical: Most frequent imputation + OneHotEncoder\n",
        "3. ✅ Fitted preprocessor on training data\n",
        "4. ✅ Transformed train and test sets\n",
        "5. ✅ Verified no data leakage\n",
        "6. ✅ Saved preprocessor for later use\n",
        "\n",
        "**Next:** Move to `modelling.ipynb` for model training and evaluation.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
