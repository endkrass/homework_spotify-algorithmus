{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train & Save Final Model\n",
        "\n",
        "This notebook covers:\n",
        "- Phase 10: Train Final Model on all Data and Save via Pickle\n",
        "\n",
        "**Note:** Run `4_pipeline_gridsearch.ipynb` first to get the optimized hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_STATE = 777\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LOADING PREPROCESSOR AND FEATURE INDICES\n",
            "============================================================\n",
            "‚úÖ Preprocessor loaded\n",
            "‚úÖ Top 6 feature indices loaded: [3, 9, 4, 12, 13, 5]\n"
          ]
        }
      ],
      "source": [
        "# Load preprocessor and top 6 feature indices\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING PREPROCESSOR AND FEATURE INDICES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with open('models/preprocessor.pkl', 'rb') as f:\n",
        "    preprocessor = pickle.load(f)\n",
        "print(\"‚úÖ Preprocessor loaded\")\n",
        "\n",
        "try:\n",
        "    with open('models/top_6_indices.pkl', 'rb') as f:\n",
        "        top_6_indices = pickle.load(f)\n",
        "    print(f\"‚úÖ Top 6 feature indices loaded: {top_6_indices}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ö†Ô∏è top_6_indices.pkl not found. Run modelling.ipynb first!\")\n",
        "    raise\n",
        "\n",
        "# Reload and prepare all data\n",
        "df = pd.read_csv('data/spotify-tracks.csv')\n",
        "columns_to_drop = ['spotify_id', 'name', 'artists', 'album_name', 'album_release_date',\n",
        "                   'popular_in_country', 'mode', 'is_explicit', 'release_year', \n",
        "                   'key', 'time_signature', 'release_month', 'duration_ms', 'popularity']\n",
        "df_clean = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "target = 'energy'\n",
        "y = df_clean[target].copy()\n",
        "X = df_clean.drop(columns=[target]).copy()\n",
        "\n",
        "# Feature engineering\n",
        "X_engineered = X.copy()\n",
        "X_engineered['loudness_tempo'] = X_engineered['loudness'] * X_engineered['tempo']\n",
        "X_engineered['danceability_valence'] = X_engineered['danceability'] * X_engineered['valence']\n",
        "X_engineered['loudness_danceability'] = X_engineered['loudness'] * X_engineered['danceability']\n",
        "X_engineered['tempo_valence'] = X_engineered['tempo'] * X_engineered['valence']\n",
        "X = X_engineered.copy()\n",
        "\n",
        "# Preprocess all data\n",
        "X_all_processed = preprocessor.fit_transform(X)\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "used_original_features = list(X.columns.intersection(df.columns))  # Original CSV columns still in use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training on all data (21585 samples)...\n",
            "\n",
            "============================================================\n",
            "‚úÖ FINAL MODEL trained successfully on all data!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Feature selector transformer (same as in pipeline_gridsearch.ipynb)\n",
        "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Selects top N features\"\"\"\n",
        "    def __init__(self, feature_indices):\n",
        "        self.feature_indices = feature_indices\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        return X[:, self.feature_indices]\n",
        "\n",
        "# Select top 6 features\n",
        "X_all_top6 = X_all_processed[:, top_6_indices]\n",
        "\n",
        "\n",
        "\n",
        "# Create final model with optimized hyperparameters from grid search\n",
        "# These are the best parameters found in pipeline_gridsearch.ipynb\n",
        "# Random Forest Regressor outperformed Gradient Boosting\n",
        "# Optimal Hyperparameters:\n",
        "#  - n_estimators: 600\n",
        "#  - max_depth: None (unlimited)\n",
        "#  - max_features: 'sqrt'\n",
        "#  - min_samples_split: 2\n",
        "#  - min_samples_leaf: 1\n",
        "\n",
        "#######################################################################################################################################\n",
        "## Attention!! Using reduced n_estimators and max_depth for final model to save space (model.pkl was 1,3GB with 600 trees and unlimited max_depth)##\n",
        "######################################################################################################################################\n",
        "\n",
        "# Used Hyperparameters:\n",
        "#  - n_estimators: 200\n",
        "#  - max_depth: 10\n",
        "#  - max_features: 'sqrt'\n",
        "#  - min_samples_split: 2\n",
        "#  - min_samples_leaf: 1\n",
        "\n",
        "final_model = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    max_features='sqrt',\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining on all data ({len(X_all_processed)} samples)...\")\n",
        "final_model.fit(X_all_top6, y)\n",
        "\n",
        "# Evaluate on all data\n",
        "y_pred_all = final_model.predict(X_all_top6)\n",
        "r2_all = r2_score(y, y_pred_all)\n",
        "rmse_all = np.sqrt(mean_squared_error(y, y_pred_all))\n",
        "mae_all = mean_absolute_error(y, y_pred_all)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ FINAL MODEL trained successfully on all data!\")\n",
        "print(\"=\" * 60)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "SAVING MODEL\n",
            "============================================================\n",
            "\n",
            "‚úÖ Model saved to 'models/model.pkl'\n",
            "\n",
            "Model package includes:\n",
            "  - Trained Random Forest model\n",
            "  - Top 6 feature indices\n",
            "  - Preprocessor\n",
            "  - Performance metrics\n",
            "  - Optimized hyperparameters\n",
            "  - Grid search results summary\n"
          ]
        }
      ],
      "source": [
        "# Save model package\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAVING MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "model_package = {\n",
        "    'model': final_model,\n",
        "    'top_6_indices': top_6_indices,\n",
        "    'feature_names': feature_names,\n",
        "    'preprocessor': preprocessor,\n",
        "    'model_type': type(final_model).__name__,\n",
        "    'r2_score': r2_all,\n",
        "    'rmse': rmse_all,\n",
        "    'mae': mae_all,\n",
        "    'hyperparameters': {\n",
        "        'n_estimators': 200,\n",
        "        'max_depth': 20,\n",
        "        'max_features': 'sqrt',\n",
        "        'min_samples_split': 2,\n",
        "        'min_samples_leaf': 1\n",
        "    },\n",
        "    'grid_search_info': {\n",
        "        'best_cv_r2': 0.6968,\n",
        "        'test_r2': 0.7131,\n",
        "        'model_selected': 'RandomForestRegressor'\n",
        "    }\n",
        "}\n",
        "\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "with open('models/model.pkl', 'wb') as f:\n",
        "    pickle.dump(model_package, f)\n",
        "\n",
        "print(\"\\n‚úÖ Model saved to 'models/model.pkl'\")\n",
        "print(\"\\nModel package includes:\")\n",
        "print(\"  - Trained Random Forest model\")\n",
        "print(\"  - Top 6 feature indices\")\n",
        "print(\"  - Preprocessor\")\n",
        "print(\"  - Performance metrics\")\n",
        "print(\"  - Optimized hyperparameters\")\n",
        "print(\"  - Grid search results summary\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING MODEL LOADING\n",
            "============================================================\n",
            "‚úÖ Model loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Test loading\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TESTING MODEL LOADING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with open('models/model.pkl', 'rb') as f:\n",
        "    loaded_package = pickle.load(f)\n",
        "\n",
        "loaded_model = loaded_package['model']\n",
        "loaded_indices = loaded_package['top_6_indices']\n",
        "loaded_preprocessor = loaded_package['preprocessor']\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PREDICTION SANITY CHECK (QUALITATIVE)\n",
            "============================================================\n",
            "Note: The following examples are for demonstration only.\n",
            "They do NOT represent an unbiased performance evaluation.\n",
            "\n",
            "Index    Actual     Predicted    Abs Error \n",
            "--------------------------------------------------\n",
            "14720    0.8210     0.7835       0.0375    \n",
            "18254    0.7170     0.5851       0.1319    \n",
            "912      0.7600     0.7313       0.0287    \n",
            "12732    0.5480     0.5059       0.0421    \n",
            "2003     0.6720     0.5421       0.1299    \n",
            "\n",
            "Prediction range check:\n",
            "  Min prediction: 0.506\n",
            "  Max prediction: 0.783\n",
            "‚úÖ All predictions within valid range [0, 1]\n",
            "\n",
            "Summary (demonstration only):\n",
            "  Mean absolute error: 0.0740\n",
            "  Max absolute error:  0.1319\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PREDICTION SANITY CHECK (QUALITATIVE)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Note: The following examples are for demonstration only.\")\n",
        "print(\"They do NOT represent an unbiased performance evaluation.\\n\")\n",
        "\n",
        "# Random sample for transparency\n",
        "test_sample = X.sample(n=5, random_state=RANDOM_STATE)\n",
        "test_indices = test_sample.index\n",
        "\n",
        "test_sample_processed = loaded_preprocessor.transform(test_sample)\n",
        "test_sample_top6 = test_sample_processed[:, loaded_indices]\n",
        "\n",
        "predictions = loaded_model.predict(test_sample_top6)\n",
        "\n",
        "print(f\"{'Index':<8} {'Actual':<10} {'Predicted':<12} {'Abs Error':<10}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for idx, actual, pred in zip(\n",
        "    test_indices,\n",
        "    y.loc[test_indices],\n",
        "    predictions\n",
        "):\n",
        "    print(f\"{idx:<8} {actual:<10.4f} {pred:<12.4f} {abs(actual - pred):<10.4f}\")\n",
        "\n",
        "print(\"\\nPrediction range check:\")\n",
        "print(f\"  Min prediction: {predictions.min():.3f}\")\n",
        "print(f\"  Max prediction: {predictions.max():.3f}\")\n",
        "\n",
        "if np.all((predictions >= 0) & (predictions <= 1)):\n",
        "    print(\"‚úÖ All predictions within valid range [0, 1]\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Some predictions outside expected range\")\n",
        "\n",
        "print(\"\\nSummary (demonstration only):\")\n",
        "print(f\"  Mean absolute error: {np.mean(np.abs(y.loc[test_indices].values - predictions)):.4f}\")\n",
        "print(f\"  Max absolute error:  {np.max(np.abs(y.loc[test_indices].values - predictions)):.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéØ Final Model Ready for Deployment\n",
        "\n",
        "The model has been:\n",
        "\n",
        "- ‚úÖ Trained on all available data  \n",
        "- ‚úÖ Using optimized hyperparameters from grid search  \n",
        "- ‚úÖ Saved to `models/model.pkl`  \n",
        "- ‚úÖ Tested and verified  \n",
        "\n",
        "You can now use this model for predictions on new, unseen data.\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
